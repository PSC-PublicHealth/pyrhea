# Instruction and Details on Running RHEA for Paper Simulations

All the files related to running paper scenarios for RHEA are located in `src/tools/postprocessing`.

There are two bash important scripts: `generate_subdirs.bash` and `survey.bash` in this directory.

#### `generate_subdirs.bash`
This script creates a subdirectory for each scenario complete with all the files needed to run the full pipeline of 
RHEA simulation and analysis. It uses the scenarios defined in `scenario_names.txt` to customize the prototype files 
(`*.proto`) and generate the SLURM scripts (`*.sl`). It uses `gen_facility_yaml.py` to create the `xdro_facs.yaml` file 
(facilities using the XDRO registry, for post-processing). It uses `gen_scenario_custom_yaml.py` to create the `scenario_custom.yaml` file (see the 
`scenario_mod.py` and `scenario_custom.yaml` section below). It uses `gen_run_info.py` to generate `run_info.bash` 
(to be sourced by bash scripts needing information like total run steps). This script must be run in the parent directory for the project.

#### `survey.bash`
This script provides information on the current progress along the pipeline for each RHEA scenario. This script must be 
run in the parent directory for the project (where you run the `generate_subdirs.bash` script).

## Additional Detail on Previously Mentioned Files

#### `scenario_names.txt`
The scripts respect a '#' in the first column as a comment character.  'survey.bash' ignores lines beginning with '##' but does generate survey information for lines with a single '#'.

The format of strings is important.  Two formats are currently implemented, both for the Chicago model.  Examples of those formats are:
* SNA_CRE_Prev_13_Mile - a scenario name from the CRE Bundle + XDRO scenario set
* capture_75_search_100_add_100_eff_40 - percentage settings for capture, search, and addition to the registry and efficacy, used for the XDRO intervention scenario set

#### `scenario_mod.py` and `scenario_custom.yaml`
This pair of files customizes the baseline scenario yaml file to run RHEA using the intervention scenario parameters. 
`scenario_custom.yaml` is generated by `generate_subdirs.bash` based on the strings in `scenario_names.txt`. 
`scenario_mod.py` is run within the `run_by_array.sl` script to configure the scenario at runtime. `scenario_mod.py` 
also copies the cache for you (see the Moving the Cache Directory section below).

#### `runs_by_array.sl` (`runs_by_array.proto`)
The final command in this script is the Python command that runs RHEA. It uses the `-c` command line flag to run 
`scenario_mod.py` using the `scenario_custom.yaml` file that is generated by `generate_subdirs.bash`.

#### Moving the Cache Directory
RHEA uses the LMDB database library to store a cached copy of the state of the 'freeze dried' agents in the community.  LMDB data can only be accessed by multiple processes if the individual data files can be locked, and parallel filesystems generallly do not support global file locks for reasons of run-time overhead.  The /pylon5 filesystem is such a parallel filesystem.  Thus multiple copies of RHEA running different instances on different compute nodes cannot simultaneously access the agent data cache.  To avoid this problem, separate copies of the cache are made at run time.  The copying is done by 'scenario_mod.py'.  It is possible for all instances of RHEA running on a single node to share a single copy of the cache on that node's local filesystem.  Alternately, every instance of RHEA can create its very own copy in a directory on the parallel filesystem.  Since each such copy has only a single user, no collisions result.  We generally take the latter approach for simplicity, even though it involves creating more temporary copies.  These copies must be cleaned up manually after the ensemble of runs is complete.

## Pipeline for Running RHEA
1. Run `generate_subdirs.bash` in the parent directory for your scenarios. This will generate the subdirectories for 
your various scenarios.
2. `cd` into each subdirectory and submit a job using the `runs_by_array.sl` SLURM script. This will launch 51 
realization of RHEA for the specific scenario described by the subdirectory. This script will generate a `.bcz_0` data directory (bcolz format) 
for each realization.
3. Check that your `.bcz_0` directories are good. This is done by checking the output of `survey.bash` to ensure you have 51 
completed realizations and by looking at the log file (`pyrhea_*_out.out` file) of each realization to make sure the sizes are all 
consistent.
4. In each subdirectory, submit a job using the `gen_counts_parallel.sl` SLURM script. This will convert the `.bcz` 
files to `.mpz` files. The `.mpz` files are denser, quicker to read, and easier to clean up.
5. In each subdirectory, submit a job using the `gen_csvfiles_parallel.sl` SLURM script. This will generate the “raw” 
csv files from the `.mpz` files. The script will generate 3 csv files per run (TODO: What are their names?). These 
files are the first files that are handed over to the modeling team.
6. In each subdirectory, submit a job using the `gen_costs_parallel.sl` SLURM script. This will generate costing 
information (TODO: One csv file per realization? What is the output named?) using the `.mpz` files.
7. TODO: Finish this section -> There is a bash script and an R script in `src/tools/postprocessing/R/` for generating 
confidence intervals. The process for running the R script is not entirely automated yet, and instructions for running 
the script will be covered in a later tutorial.
8. When everything is done and signed off on by the modeling team, submit, in each subdirectory, a job using the 
`cleanup.sl` SLURM script. This will compress everything (TODO: What is everything?) and delete the `.bcz_0` directories. It is 
still necessary to manually delete all of the copies of the cache (the locations of which are specified in 'scenario_mod.py').

#### What to Do If Any of Your Realizations Fail
By convention we attempt to run 51 realizations in an ensemble, and are satisfied with 49 valid results.  This number is just a convention and is not required by any script, but is used in the following description for completeness.

If some realizations fail during step 3, fewer than 49 complete realizations may be created. If some 
of your realizations fail for any reason and you wind up with fewer than 49 completed realizations (look at the output 
of survey.bash to see), you will need to run additional realizations. To do so, you will need to manually edit the 
`runs_by_array.sl` SLURM script. In the `SBATCH` lines at the top of the script, locate the `#SBATCH --array=0-50` 
line. Change this line to be `#SBATCH --array=100-1XX` where `XX` is the number of realizations you need. So, if you 
had 12 runs fail, that line in your file should read `#SBATCH --array=100-112`. Then, submit a job using the 
`runs_by_array.sl` SLURM script as you did in Step 2. Repeat this process until you have at least 49 completed 
realizations for every scenario. After this, you will need to return to your top level directory and run the 
`generate_subdirs.bash` script again. This will regenerate the SLURM scripts with the correct range based on the actual 
number of runs that have been completed.

#### Other Pipeline Related Generated SLURM Scripts
* `run_taumod.sl`: To be covered in a later tutorial
* `run_diagnostics.sl`: Makes a bunch of plots. Uses the `.mpz` files and an `expected.pkl` file. `expected.pkl` is a 
copy of the pkl file generated by your last taumod run. TODO: Fill in a little more after covering taumod.
