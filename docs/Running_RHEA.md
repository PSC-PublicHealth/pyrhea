# Instruction and Details on Running RHEA for Paper Simulations

All the files related to running paper scenarios for RHEA are located in `src/tools/postprocessing`.

There are two bash important scripts: `generate_subdirs.bash` and `survey.bash` in this directory.

#### `generate_subdirs.bash`
This script creates a subdirectory for each scenario complete with all the files needed to run the full pipeline of 
RHEA simulation and analysis. It uses the scenarios defined in `scenario_names.txt` to customize the prototype files 
(`*.proto`) and generate the SLURM scripts (`*.sl`). It uses `gen_facility_yaml.py` to create the `xdro_fact.yaml` file 
(TODO: What is this for?). It uses `gen_scenario_custom_yaml.py` to create the `scenario_custom.yaml` file (see the 
`scenario_mod.py` and `scenario_custom.yaml` section below). It uses `gen_run_info.py` to generate `run_info.bash` 
(TODO: What is this for?). This script must be run in the parent directory for the project.

#### `survey.bash`
This script provides information on the current progress along the pipeline for each RHEA scenario. This script must be 
run in the parent directory for the project (where you run the `generate_subdirs.bash` script).

## Additional Detail on Previously Mentioned Files

#### `scenario_names.txt`
The format of the strings is important. TODO: Get detail on the format

#### `scenario_mod.py` and `scenario_custom.yaml`
This pair of files customizes the baseline scenario yaml file to run RHEA using the intervention scenario parameters. 
`scenario_custom.yaml` is generated by `generate_subdirs.bash` based on the strings in `scenario_names.txt`. 
`scenario_mod.py` is run within the `run_by_array.sl` script to configure the scenario at runtime. `scenario_mod.py` 
also copies the cache for you (see the Moving the Cache Directory section below).

#### `run_by_array.sl` (`run_by_array.proto`)
The final command in this script is the Python command that runs RHEA. It uses the `-c` command line flag to run 
`scenario_mod.py` using the `scenario_custom.yaml` file that is generated by `generate_subdirs.bash`.

#### Moving the Cache Directory
There’s something about some filesystem that causes access to RHEA’s cache information to be serialized across the 
multiple realizations causing the realizations themselves to essentially run serially. To remedy this, the 
`scenario_mod.py` script copies the cache for you somewhere else. This results in one copy of the cache for every 
realization of RHEA. Eventually these need to be cleaned up. This is currently done manually. TODO: Get the actual 
reason for this.

## Pipeline for Running RHEA
1. Run `generate_subdirs.bash` in the parent directory for your scenarios. This will generate the subdirectories for 
your various scenarios.
2. `cd` into each subdirectory and submit a job using the `runs_by_array.sl` SLURM script. This will launch 51 
realization of RHEA for the specific scenario described by the subdirectory. This script will generate a `.bcz` file 
for each realization.
3. Check that your `.bcz` files are good. This is done by checking the output of `survey.bash` to ensure you have 51 
completed realizations and by looking at the log file (`.out` file) of each realization to make sure the sizes are all 
consistent.
4. In each subdirectory, submit a job using the `gen_counts_parallel.sl` SLURM script. This will convert the `.bcz` 
files to `.mpz` files. The `.mpz` files are denser, quicker to read, and easier to clean up.
5. In each subdirectory, submit a job using the `gen_csvfiles_parallel.sl` SLURM script. This will generate the “raw” 
csv files from the `.mpz` files. The script will generate 3 csv files per run (TODO: What are their names?). These 
files are the first files that are handed over to the modeling team.
6. In each subdirectory, submit a job using the `gen_costs_parallel.sl` SLURM script. This will generate costing 
information (TODO: One csv file per realization? What is the output named?) using the `.mpz` files.
7. TODO: Finish this section -> There is a bash script and an R script in `src/tools/postprocessing/R/` for generating 
confidence intervals. The process for running the R script is not entirely automated yet, and instructions for running 
the script will be covered in a later tutorial.
8. When everything is done and signed off on by the modeling team, submit, in each subdirectory, a job using the 
`cleanup.sl` SLURM script. This will compress everything (TODO: What is everything?) and delete the `.bcz` files. It is 
still necessary to manually delete all of the copies of the cache (TODO: Where to find the copies?).

#### What to Do If Any of Your Realizations Fail
All of the steps from 4 onwards in the pipeline expect you to have at least 49 realization of RHEA completed. If some 
of your realizations fail for any reason and you wind up with fewer than 49 completed realizations (look at the output 
of survey.bash to see), you will need to run additional realizations. To do so, you will need to manually edit the 
`runs_by_array.sl` SLURM script. In the `SBATCH` lines at the top of the script, locate the `#SBATCH --array=0-50` 
line. Change this line to be `#SBATCH --array=100-1XX` where `XX` is the number of realizations you need. So, if you 
had 12 runs fail, that line in your file should read `#SBATCH --array=100-112`. Then, submit a job using the 
`runs_by_array.sl` SLURM script as you did in Step 2. Repeat this process until you have at least 49 completed 
realizations for every scenario. After this, you will need to return to your top level directory and run the 
`generate_subdirs.bash` script again. This will regenerate the SLURM scripts with the correct range based on the actual 
number of runs that have been completed.

#### Other Pipeline Related Generated SLURM Scripts
* `run_taumod.sl`: To be covered in a later tutorial
* `run_diagnostics.sl`: Makes a bunch of plots. Uses the `.mpz` files and an `expected.pkl` file. `expected.pkl` is a 
copy of the pkl file generated by your last taumod run. TODO: Fill in a little more after covering taumod.
